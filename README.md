# ğŸ¤– Model Architecture: Transformer Macro Autoencoder

Transformer ê¸°ë°˜ì˜ ì˜¤í† ì¸ì½”ë”(Autoencoder) êµ¬ì¡°
- ì •ìƒ íŒ¨í„´: ëª¨ë¸ì´ ë†’ì€ ì •í™•ë„ë¡œ ë³µì›í•˜ì—¬ ì¬êµ¬ì„± ì˜¤ì°¨ 0ì— ìˆ˜ë ´í•©ë‹ˆë‹¤.
- ì´ìƒ íŒ¨í„´(ë§¤í¬ë¡œ): ëª¨ë¸ì´ í•™ìŠµí•˜ì§€ ëª»í•œ íŒ¨í„´ì´ë¯€ë¡œ ë³µì› ëŠ¥ë ¥ì´ ë–¨ì–´ì ¸ ì¬êµ¬ì„± ì˜¤ì°¨ê°€ ë†’ê²Œ ë°œìƒí•©ã„´ë‹¤.

- Feature Embedding : 5ì°¨ì›ì˜ ì…ë ¥ í”¼ì²˜(x, y, dist ë“±)ë¥¼ d_model(64ì°¨ì›)ì˜ ê³ ì°¨ì› ë²¡í„°ë¡œ í™•ì¥í•˜ì—¬ ë³µì¡í•œ ìƒê´€ê´€ê³„ë¥¼ í•™ìŠµí•  ì¤€ë¹„ë¥¼ í•©ë‹ˆë‹¤.
- Positional Encoding : TransformerëŠ” RNNê³¼ ë‹¬ë¦¬ ìˆœì„œ ì •ë³´ê°€ ì—†ìœ¼ë¯€ë¡œ, ì‹œí€€ìŠ¤ ë‚´ ê° ìœ„ì¹˜ ì •ë³´($1^{st}, 2^{nd}, ...$)ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë²¡í„°ë¥¼ ë”í•´ì¤ë‹ˆë‹¤.
- Transformer Encoder : Multi-Head Self-Attention ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ ì‹œí€€ìŠ¤ ì „ì²´ë¥¼ ë™ì‹œì— í›‘ìœ¼ë©°, ê³¼ê±°ì˜ ì›€ì§ì„ì´ í˜„ì¬ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ íŒŒì•…í•©ë‹ˆë‹¤.
- Linear Decoder : ì¸ì½”ë”ê°€ ë½‘ì•„ë‚¸ ì¶”ìƒì ì¸ íŠ¹ì§•ë“¤ì„ ë‹¤ì‹œ ì›ë˜ì˜ 5ê°œ í”¼ì²˜ ì°¨ì›ìœ¼ë¡œ ë³µì›í•©ë‹ˆë‹¤.

Detection Logic
- Normal Patterns: The model reconstructs these with high precision, causing the reconstruction error to converge to zero.
- Anomalous Patterns (Macro): Since these are patterns the model has not encountered during training, the reconstruction capability decreases, resulting in a high reconstruction error.

- Feature Embedding: Expands the 5-dimensional input features (e.g., $x, y, dist$) into a high-dimensional vector of $d_{model}$ (64 dimensions) to prepare the model for learning complex correlations.
- Positional Encoding: Since Transformers do not inherently process sequential order like RNNs, this adds vectors that represent the positional information ($1^{st}, 2^{nd}, \dots$) within the sequence.
- Transformer Encoder: Utilizes the Multi-Head Self-Attention mechanism to scan the entire sequence simultaneously, capturing how past movements influence the present state.
- Linear Decoder: Reconstructs the abstract features extracted by the encoder back into the original 5-feature dimensions.

![Architecture Diagram](./public/Architecture.png)

# ì •ì‹ 1.0.0 ë²„ì „ ì¶œì‹œ ì „ê¹Œì§€ ê¸°ëŠ¥ ê°œì„  ë° ì•ˆì •í™”ë¥¼ ìœ„í•´ ë¹ˆë²ˆí•œ ì—…ë°ì´íŠ¸ê°€ ì§„í–‰ë  ì˜ˆì •ì…ë‹ˆë‹¤.
# Frequent updates are expected for feature enhancement and stabilization until the official v1.0.0 release.

# ğŸš€ Macro Detector Update (Ver 0.0.4)

## ğŸ“ Change Log (KO)
* **ëª¨ë¸ ì—…ê·¸ë ˆì´ë“œ**: ìœ ì € ë°ì´í„° ì¦ê°€ì— ëŒ€ì‘í•˜ì—¬ `d_model` ì°¨ì› í™•ì¥ ë° ì¬í›ˆë ¨ ìˆ˜í–‰
* **í†µì‹  ì•ˆì •í™”**: ì›¹ì†Œì¼“(WebSocket) ì—°ê²° ë° ìŠ¤íŠ¸ë¦¬ë° ì•ˆì •ì„± ê°•í™”
* **ìŠ¤í‚¤ë§ˆ ì •ì˜**: `app.models.MouseDetectorSocket.py` ë‚´ Request/Response ëª¨ë¸ ì •ë¦½
* **í…ŒìŠ¤íŠ¸ ë„êµ¬**: í”„ë¡ íŠ¸ì—”ë“œì™€ ë°±ì—”ë“œ í†µí•© ì›¹ í…ŒìŠ¤íŠ¸ í™˜ê²½(`test_web`) ì¶”ê°€

```
# backend
python -m uvicorn main:app --host 0.0.0.0 --port 8300 --reload

# frontend
npx vite
```

## ğŸ“ Change Log (EN)
* **Model Upgrade**: Re-trained the model with an expanded `d_model` to accommodate increasing user data.
* **WebSocket Stability**: Enhanced stability for real-time WebSocket communication.
* **Schema Definition**: Established `RequestBody` and `ResponseBody` in `app.models.MouseDetectorSocket.py`.
* **Testing Suite**: Provided `test_web` environment for seamless integration testing.

## ğŸ›  Data Models
**File:** `app.models.MouseDetectorSocket.py`

```
python
from pydantic import BaseModel
from typing import List, Optional

class RequestBody(BaseModel):
    id: str
    data: List[dict]

class ResponseBody(BaseModel):
    id: str
    status: int
    analysis_results: List[str]
    message: Optional[str] = None
```

---

## ğŸ“‚ Data Management
* **Database Support:** Efficient data handling using **PostgreSQL** and **JSON** formats.

## ğŸ›  Installation
* To install the required dependencies, run the following command:
  ```bash
  pip install -r requirements.txt

## ì‚¬ìš© ì„¤ëª…ì„œ (Manual)
Manual.pptx

## ì˜ìƒ
[![ì‹¤í–‰ ì˜ìƒ](https://img.youtube.com/vi/iwi31PxQc3I/0.jpg)](https://youtu.be/iwi31PxQc3I)